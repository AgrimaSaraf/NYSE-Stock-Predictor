---
title: "Stock News Sentiment Analysis"
author: "Chaitya Shah"
date: "11/3/2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_libs}
#don't delete any
library("tidyverse")
library("devtools")
library("tm")
library("SnowballC")
library("wordcloud")
library("sentiment")
library("tidytext")
library("purrr")
library("stringr")
library("tm.plugin.webmining")
```

```{r stocknews}
#read reddit news data
reddit_news = read.csv("/Users/Chaitya/fall17/DS5110/Project/Project/data/stocknews/RedditNews.csv")

reddit <- reddit_news
```

```{r analyse_news, echo=FALSE}

reddit$Date <- as.character(reddit$Date)

#filter dates

start_date <- "2011-12-01"
end_date <- "2016-01-01"

reddit <- reddit %>%
  filter(Date >= start_date & Date <= end_date)

#sort by date
reddit <- reddit %>%
  arrange(Date)

#part of data cleaning
reddit$News <- sapply(reddit$News, function(row) 
  iconv(row, "latin1", "ASCII", sub=""))

#remove 'b' from all headlines. was there probably because of bold

reddit$News = gsub('b"', '"', reddit$News)
reddit$News = gsub("b'", "'", reddit$News)

reddit_clean <- reddit

#add id to spread based on date and news.
#Gives 25 headlines per day

reddit_clean <- reddit_clean %>%
  group_by(Date) %>%
  mutate(id=paste0("News_",1:n()))

reddit_clean <- reddit_clean %>%
  spread(key = "id", value = "News")

#keep first 25 headlines
reddit_clean <- reddit_clean %>%
  select(Date, paste0("News_",1:25))
```


```{r package_sentiment}

#package sentiment
#https://github.com/andrie/sentiment

# example
# sentiment(c("This is a great project"))

reddit_scores <- data.frame()

for(i in 1:25){
  reddit_clean[paste0("Score_", i)] <- sentiment(c(reddit_clean[[paste0("News_", i)]]))
}

#use loop here

reddit_scores <- reddit_clean %>%
  transmute(Total = sum(Score_1 + Score_2 + Score_3 + Score_4 +
                          Score_5 + Score_6 + Score_7 + Score_8 +
                          Score_9 + Score_10 + Score_11 + Score_12 +
                          Score_13 + Score_14 + Score_15 + Score_16 +
                          Score_17 + Score_18 + Score_19 + Score_20 +
                          Score_21 + Score_22 + Score_23 + Score_24 + 
                          Score_25, na.rm=TRUE))


```

```{r reddit_tokens}
# tokenization method but
# getting negative values for all 
reddit_tokens <- reddit_clean %>%
  unnest_tokens(word, News_1) 


reddit4 <- reddit

# get individual words
news_tokens <- reddit4 %>%
  unnest_tokens(word, News)

# convert row names to integer formats
row_names <- row.names(news_tokens)
news_tokens$row_names <- row_names

news_tokens$row_names <- as.integer(news_tokens$row_names)

news_tokens_test <- news_tokens[1:10000,]
news_tokens_test$row_names <- as.integer(news_tokens_test$row_names)

news_tf_idf <- news_tokens_test %>%
  count(Date, word) %>%
  filter(!str_detect(word, "\\d+")) %>%
  bind_tf_idf(word, Date, n) %>%
  arrange(-tf_idf)

news_tokens_test %>%
  anti_join(stop_words, by = "word") %>%
  count(word, Date, sort = TRUE) %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(word) %>%
  summarize(contribution = sum(n * score)) %>%
  top_n(20, abs(contribution)) %>%
  mutate(word = reorder(word, contribution)) %>%
  ggplot(aes(word, contribution)) +
  geom_col() +
  coord_flip() +
  labs(y = "Frequency of word * AFINN score")

news_tokens_test %>%
  count(word) %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ sentiment, scales = "free") +
  ylab("Frequency of this word in the recent financial headlines")

news_sentiment <- news_tokens_test %>%
  inner_join(get_sentiments("loughran"), by = "word") %>%
  count(sentiment, Date) %>%
  spread(sentiment, n, fill = 0)

news_result_table <- news_sentiment %>%
  mutate(score = (positive - negative) / (positive + negative)) %>%
  mutate(Date = reorder(Date, score))

```
